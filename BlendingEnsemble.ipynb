{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Project Topic: Application of Ensemble Learning (Blending) in developing a Predictive model for Nvidia Stock price uptrend move.\n",
    "### - Caleb Fowowe"
   ],
   "id": "951328478a3674e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Import Libraries",
   "id": "b74e75801e9a587c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T20:07:48.441151Z",
     "start_time": "2024-08-17T20:07:44.480792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from src.utils_data_processing import LoadData, cwts, getpath\n",
    "from src.utils_features_engineering import (FeaturesCreation, FeaturesTransformation, FeaturesSelection)\n",
    "from src.utils_model_and_tuning import Blending, HpTuning\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import quantstats as qs\n",
    "from datetime import datetime\n",
    "\n",
    "# Creates a folder for saving of code graphics and trading strategy report.\n",
    "output_path = getpath()"
   ],
   "id": "450f9a5f052c225",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Data, EDA, Fix Null Data, and Plot Candlestick",
   "id": "1a289f098b02c4b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_files = {'files': ['NVDA', 'VVIX_History', 'USCPI', 'USGDP', 'FedFundRate', '2yrTreasury', '10yrTreasury']} #File names of the data as dictionary values\n",
    "\n",
    "time_period = ['2008', '2024'] #specifies a period range, in the case provided data is goes back than required.\n",
    "company_name = data_files['files'][0] #Extract the stock or company name here\n",
    "\n",
    "ldata = LoadData(*time_period, **data_files) #instantiate the class\n",
    "\n",
    "df = ldata.joinData() #merge all data together to form a single dataframe."
   ],
   "id": "25281ee883e45473",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Exploratory Data Analysis (EDA)",
   "id": "cfe273e7e563c53f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Check and fix null data (null data)\n",
    "print(ldata.checkNullData(df)) #check for missing data in the dataset\n",
    "df = ldata.fixNullData(df, method='bfill') #Intiall step of fixing missing data, based on the earlier stated method, here backfill is method is used tp backfill quarterly, and monthly macrodata\n",
    "print(ldata.checkNullData(df)) #check for null data after fixing null data\n",
    "df = ldata.fixNullData(df, method='knnimpute') #fix future data that are not available yet, can drop rows to choose, knn_impute method was used here.\n",
    "# ldata.plotCandleStick(df)\n",
    "ldata.plotPrices(df)\n",
    "df.tail()"
   ],
   "id": "c9b13bda9718610e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.describe()",
   "id": "fd8c05af232fbd76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "92f256b77197a8cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "50e222c9c7b8a460",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature Engineering\n",
    "\n",
    "##### FeaturesEngineering Class with the entire FeaturesCreation, FeaturesTransformation, and FeaturesSelections sub-classes"
   ],
   "id": "bfe21c11ea24f4f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Feature Creation/Extraction",
   "id": "fd6dd79d579a8fc9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Define the parameters to be used in the target variable (y)/ Label",
   "id": "72c6383fb83643e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The target is a trend and volatility play which creates a signal when the return over a short period of time (5days), crosses over the return trend over a relative medium period (10days). However, there is an outperformance threshold (hurdle) over which the 5-day return must outperform the medium_period return before it is charactersized as a condtion. The other condition is a volatility play where the standard deviation of the short period return is less than the upper boundary standard deviation of the medium period return. The upper boundary of the medium period returns standard deviation is characterized as 2-standard deviations from the mean\n",
    "\n",
    "# Target parameters\n",
    "short_prd= 5 \n",
    "medium_prd = 10\n",
    "upper_std=2 \n",
    "lower_std = 1 \n",
    "hurdle = 0.005"
   ],
   "id": "3f935138c82de30c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Generate Features - all features (pandas-ta library)",
   "id": "8c79b379725478b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Instantiate the features creation subclass with the dataframe containing the cleaned data, and the testsize as input parameters. The testsize is opional and has a default value of 20%.\n",
    "# The testsize is provided during the Features engineering process because it will be used during features selection step.\n",
    "\n",
    "feat_df = FeaturesCreation(df, short_prd, medium_prd, upper_std, lower_std, hurdle) #Instantiate the FeaturesCreation subclass providing the dataframe and the target parametres as inputs\n",
    "new_ft = feat_df.create_all_features(fundamental_features=True, macro_features=True) # If the Fundamental and Macro ratios are provided, genrates a feature set of:\n",
    "#1. Company fundamentals-related Features (Requires specific column label)\n",
    "#2. Macro-economic related features (Requires specific column label)\n",
    "#3. Technical Indicator features (based on pandas ta-library) (Requires specific column label 'Open', 'High', 'Low', 'Close', 'Volume')\n",
    "\n",
    "# The ohlcv columns are dropped after using them in the generation of the technical indicators. Below is a preview of the first five row of the 320features including both, macroeconomic, fundamental and technical indicators.\n",
    "new_ft.head()"
   ],
   "id": "b75aff6dcc13472d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "new_ft.shape",
   "id": "85771ee974f0ffb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Feature Transformation & Selection",
   "id": "d56dc0b66c24a365"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Transform day feature column",
   "id": "bcdc92bbfc51cf7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prior to starting to features selection process, the days features which consist of trading days (Monday - Friday), is transformed, to two features.\n",
    "feat_transform = FeaturesTransformation(new_ft) #Instantiate the FeaturesTransformation subclass providing the dataset with the generated features as input\n",
    "new_ft2 = feat_transform.transformDaysColumn() # Invoke the transformDaysColumn method which has the defined DaysTransformer subclass to transform the 'days' column. The column named 'days' must be in the input dataset for this method to execute. The whole feature set with the transformed 'days' column is stored in the 'new_ft2' variable."
   ],
   "id": "b2477c5c6b8c2edf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###### To optimize the dataframe performance, the all features outside the target column are convereted to 'float64', with the target variable column converted to 'int16' datatype",
   "id": "bf1e59968c3b6d0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "new_ft2 = new_ft2.astype('float64')\n",
    "new_ft2['predict'] = new_ft2['predict'].values.astype('int16')"
   ],
   "id": "204b9fd2ee98a458",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "new_ft2",
   "id": "2b42fd8e4064f228",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "14988e3172ea2fae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Feature Selection",
   "id": "a9f3ca6b353e7526"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###### Feature selection - Wrapper Method: Boruta and Recursive Forward Elimination (RFE)",
   "id": "38eb88883ed5a1e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "feat_select = FeaturesSelection(new_ft2, testsize = 0.20) #instantiate the FeaturesSelection subclass, providing dataframe from above, with the days column transformed as required input parameters, and the testsize as an optional input parameter as well. Default testsize is 0.20.\n",
    "feat1 = feat_select.wrapper_boruta(max_iter=200) # Invoke the wrapper_boruta method within the FeaturesSelection subclass.\n",
    "\n",
    "#The wrapper_boruta method"
   ],
   "id": "709bf88ca9060da4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Feature selection - Filtering Method: Addressing Multicollinearity among features",
   "id": "9c92d52365552218"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###### using the same Feature Selection class, specify the correlation coefficient Threshold of choice. (The projected tested correlation in the 0.60 - 0.90) ranges.",
   "id": "b8020417f31f0882"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Call the filter_correlation method, within the class providing it with the desired correlation threshold.\n",
    "# The multicollinearity steps follows the Boruta and RFE intersection steps. Hence, there is no need to specify the dataframe, the code has designed such that it already takes as input the dataframe which contains the features output of Boruta and RFE intersection. \n",
    "# However, for testing purposes, there's an optionality to provide the function with both correlation coefficient and dataframe, and it will filter for multicollinearitu among features.\n",
    "filtered_feature = feat_select.filter_correlation(corr_coeff=0.70)  "
   ],
   "id": "f6550bd52e40e589",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data3 = new_ft2[filtered_feature]\n",
    "data3['predict'] = new_ft2['predict'].values.astype('int')\n",
    "\n",
    "data3.head()"
   ],
   "id": "dc0e1d8ba9b8eb8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d2e61ec40f7dec0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Ensemble Model - Blending Ensemble",
   "id": "749f9a56876c03bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Initial parameterization of basemodels and metamodel",
   "id": "60167b724c42cc15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cls_weight = cwts(data3)\n",
    "\n",
    "lr_params = {'random_state': 1, 'class_weight': cls_weight}\n",
    "lr = LogisticRegression(**lr_params)\n",
    "\n",
    "dt_params = {'class_weight': cls_weight, 'random_state': 1}\n",
    "dt = DecisionTreeClassifier(**dt_params)\n",
    "\n",
    "knn_params = {'algorithm': 'auto', 'n_jobs': -1}\n",
    "knn = KNeighborsClassifier(**knn_params)\n",
    "\n",
    "bayes_params = {}\n",
    "bayes = GaussianNB()\n",
    "bayes.set_params(**bayes_params)\n",
    "\n",
    "svc_params = {'class_weight': cls_weight,'random_state': 1, 'probability': True}\n",
    "svc = SVC(**svc_params)\n",
    "\n",
    "basemodels = {'lr': lr, 'dte': dt, 'knn': knn, 'bayes': bayes, 'svc': svc}\n",
    "\n",
    "xgb_params = {'n_jobs': -1, 'class_weight': cls_weight, 'random_state': 1, 'verbose': 1}\n",
    "xgb = XGBClassifier(**xgb_params)\n",
    "\n",
    "blender = xgb"
   ],
   "id": "ed3df1b4bdfea0e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b41db91f56f0b56b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Initial run of the blending model",
   "id": "ae39b9e22fd7de04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Separate final X and y - Features and target\n",
    "X_final = data3.iloc[:,:-1].values\n",
    "y_final = data3.iloc[:,-1].values\n",
    "\n",
    "Blnd = Blending(X_final, y_final, basemodels, blender, valsize=0.20)\n",
    "acc, f1score, ypred, yprob, yfull = Blnd.runBlendingEnsemble()\n",
    "\n",
    "print(f\"Accuracy Score: {acc: .1%}, f1score: {f1score:.1%}\")"
   ],
   "id": "d99066c02869beb7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Hyperparamter Tuning",
   "id": "a32afef0443b4782"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Instantiate tuning\n",
    "tune_model = HpTuning(X_final, y_final, n_trials=40)\n",
    "tuned_lr, tuned_dt, tuned_svc, tuned_knn, tuned_bayes, tuned_xgb = tune_model.optimize_lr(), tune_model.optimize_dt(), tune_model.optimize_svc(), tune_model.optimize_knn(), tune_model.optimize_bayes(), tune_model.optimize_xgb()\n",
    "\n",
    "print(\"optimal_lr:\", tuned_lr.values, \"\\t\",\"optimal_dt:\", tuned_dt.values, \"\\t\", \"optimal_svc:\", tuned_svc.values, \"\\t\", \"optimal_knn:\", tuned_knn.values, \"\\t\", \"optimal_bayes:\", tuned_bayes.values, \"\\t\", \"optimal_xgb:\", tuned_xgb.values)"
   ],
   "id": "1eaafbac0e56f8c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9f8ab6b21166af99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Run Ensemble Model with tuned parameters",
   "id": "f58e967bc9ef8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Update the initial parameters dictionary with the hyperparameter tuning parameter values",
   "id": "a3e68814703af3f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lr_params.update(tuned_lr.params)\n",
    "lr = LogisticRegression(**lr_params)\n",
    "\n",
    "dt_params.update(tuned_dt.params)\n",
    "dt = DecisionTreeClassifier(**dt_params)\n",
    "\n",
    "knn_params.update(tuned_knn.params)\n",
    "knn = KNeighborsClassifier(**knn_params)\n",
    "\n",
    "bayes_params = {}\n",
    "bayes = GaussianNB()\n",
    "bayes.set_params(**bayes_params)\n",
    "\n",
    "svc_params.update(tuned_svc.params)\n",
    "svc = SVC(**svc_params)\n",
    "\n",
    "basemod_upd = {'lre': lr, 'dte': dt, 'knn': knn, 'bayes': bayes, 'svc': svc}\n",
    "\n",
    "xgb_params.update(tuned_xgb.params)\n",
    "xgb = XGBClassifier(**xgb_params)\n",
    "\n",
    "blender_upd = xgb"
   ],
   "id": "daa10fc85132871",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "26c9fd52d3760c2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### TunedModels Output",
   "id": "a95dc111207d52a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Blnd = Blending(X_final, y_final, basemod_upd, blender_upd, valsize=0.20)\n",
    "acc_tuned, f1score_tuned, ypred_tuned, yprob_tuned, yfull_tuned = Blnd.runBlendingEnsemble()\n",
    "\n",
    "print(f\"Accuracy Score: {acc_tuned: .1%}, f1score: {f1score_tuned:.1%}\")"
   ],
   "id": "c205e3f2658457bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4489b142a65b0c9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Backtest/Strategy Evaluation",
   "id": "6f6a7bdf8131e8ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Trading Strategy - full period backtest",
   "id": "3191b064d8f22086"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "return_period = 1",
   "id": "c24317568f52eb7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract Close prices over the range of dates of the full model\n",
    "backtest_data = df[['Close', 'Open']][-len(yfull_tuned):]\n",
    "backtest_data['Signal'] = yfull_tuned\n",
    "\n",
    "backtest_data"
   ],
   "id": "d411cdac4a287a3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Entry logic\n",
    "backtest_data['Entry'] = np.where(backtest_data['Signal']==1, backtest_data['Close'], 0) #when the strategy signal is 1, we enter into a trade, and buy at the end of day's close.\n",
    "\n",
    "#Exit Logic\n",
    "backtest_data['Exit'] = np.where((backtest_data['Entry'] != 0) & (backtest_data['Open'].shift(-return_period) <= backtest_data['Close']),\n",
    "                         backtest_data['Open'].shift(-return_period), 0) #\n",
    "backtest_data['Exit'] = np.where((backtest_data['Entry'] != 0) & (backtest_data['Open'].shift(-return_period) > backtest_data['Close']),\n",
    "                         backtest_data['Close'].shift(-return_period), backtest_data['Exit'])\n",
    "\n",
    "# Calculate MTM\n",
    "backtest_data['P&L'] = backtest_data['Exit'] - backtest_data['Entry']\n",
    "\n",
    "# Generate Equity Curve\n",
    "backtest_data['Equity'] = backtest_data['P&L'].cumsum() + backtest_data['Close'][0]\n",
    "\n",
    "# Calculate Benchmark Return\n",
    "backtest_data['Benchmark'] = np.log(backtest_data['Close']).diff().fillna(0)\n",
    "\n",
    "# Calculate Strategy Return\n",
    "backtest_data['Strategy'] = (backtest_data['Equity']/backtest_data['Equity'].shift(return_period) - 1).fillna(0)\n",
    "backtest_data = backtest_data.iloc[:-1]"
   ],
   "id": "8bc29a584eb4c9d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Calculate the Sharpe Ratio",
   "id": "1a3952b2e7782ca9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bts = backtest_data[['Benchmark','Strategy']]\n",
    "qs.stats.sharpe(bts)"
   ],
   "id": "b85a19ff27295d93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fedcacfd98461bb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# generate report and save in the output folder\n",
    "date_time = datetime.now().strftime('%Y-%m-%d, %H%M%S')\n",
    "qs.reports.html(bts['Strategy'], bts['Benchmark'],  title=f'Strategy BackTest Report for {company_name}',\n",
    "                output=f'{output_path}/{company_name}_backtest_report_full_period-{date_time}.html')"
   ],
   "id": "468338e49c6219b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# qs.reports.full(bts['Strategy'], benchmark=bts['Benchmark'], mode='full', title=f'Strategy BackTest Report for {company_name}')",
   "id": "812f4d91e3ee2fe1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c6b4f5f7671e58ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Out of Sample Test\n",
    "##### Trading Strategy - For Test Data Period alone"
   ],
   "id": "2f26c543736928aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "btdata = df['Close'][-len(ypred):]\n",
    "btdata = btdata.to_frame()\n",
    "btdata['Benchmark'] = np.log(btdata['Close']).diff().fillna(0)\n",
    "btdata['Signal'] = ypred\n",
    "btdata['Strategy'] = btdata.Benchmark * btdata.Signal.shift(1).fillna(0)"
   ],
   "id": "46b88ddf4682f9b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# generate report and save in the output folder\n",
    "qs.reports.html(btdata['Strategy'], btdata['Benchmark'], title=f'Strategy BackTest Report for {company_name}', \n",
    "                output=f'{output_path}/{company_name}_backtesting_report_test_period-{date_time}.html')"
   ],
   "id": "280b1223ecfb13fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bto = btdata[['Benchmark','Strategy']]\n",
    "qs.stats.sharpe(bto)"
   ],
   "id": "fb15d013dfee895",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ab25c7202c08bab6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
